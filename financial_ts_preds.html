<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-F3w7mX95PdgyTmZZMECAngseQB83DfGTowi0iMjiWaeVhAn4FJkqJByhZMI3AhiU" crossorigin="anonymous">
    <!-- Custom stylesheet -->
    <link rel='stylesheet' type='text/css' href='github_pages.css'>

    <!-- Set the Title --> 
    <title>dferan -- Financial Time Series Predictions </title>
  </head>

  <body class='lighter_bg'>

    <!-- Header -->
    <header>
      <div class="collapse bg-dark" id="navbarHeader">
        <div class="container">
          <div class="row">
            <div class="col-sm-8 col-md-7 py-4">
              <h4 class="text-white" id="description">Portfolio</h4>
              <p class="text-muted">This site contains a collection of projects related to data science and information studies. Check out the READ(about)ME <a href='about_me.html'>here</a>.</p>
            </div>
            <div class="col-sm-4 offset-md-1 py-4">
              <h4 class="text-white">Contact</h4>
              <ul class="list-unstyled">
                <li><a href="https://www.linkedin.com/in/darragh-feran-697760202" target='_blank' class="text-white">Contact me on LinkedIn</a></li>
              </ul>
            </div>
          </div>
        </div>
      </div>
      <div class="navbar navbar-dark bg-dark shadow-sm">
        <div class="container">
          <a href="index.html" class="navbar-brand d-flex align-items-center"><strong>dferan</strong></a>

          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarHeader" aria-controls="navbarHeader" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span> 
          </button>
        </div>
      </div>
    </header>

    <main class="py-5 container">

      <!-- Title Card -->
      <div class="p-4 p-md-5 mb-4 text-white rounded bg-dark">
        <div class="col-md-12 px-0 text-center">
          <h1 class="display-4 fst-italic">Financial Time Series Predictions with Tensorflow</h1>
          <!-- <img class='img-fluid' src='images/rt_recommender/tomatoes_banner.png'>  -->
          <p class="lead my-3">Attempting to make stock market predictions work under the constraints of retail trading</p>
          <br>
          <p class="blog-post-meta pt-4">September 2021 </p>
        </div>
      </div>

      <!-- Article Section -->
      <div class="container">
        <article class="blog-post">

          <!-- First Section -->
          <h3 class="blog-post-title">Introduction and Motivation</h3>
          <hr>
          <p>
            This work was conducted as part of a summer project with <a href='http://dmas.lab.mcgill.ca/fung/index.htm' target='_blank'>Dr. Benjamin Fung and the Data Mining Lab at McGill University</a>. The primary goal was educational: learning to design and implement a neural network solution to a real-world problem. Neural network models tend to be best suited for large amounts of complex data. While image and video data can be a bit difficult to work with, time series data presents an easier alternative with practical real-world applications. 
          </p>

          <p>
            One application of time series modeling is stock market predictions. While trading using ANNs is common for <a href='https://www.accenture.com/ca-en/insights/financial-services/technology-advisory-neural-networks' target='_blank'>big institutions</a>, retail traders have a harder time due to various limitations. This project attempts to find ways to get the most out of neural network-based stock trading under the constraints of retail trading.  
          </p>


          <!-- Second Section -->
          <h3 class="blog-post-title">Limitations as a Retail Trader </h3>
          <hr>

          <p>
            Training a machine-learning model requires large amounts of high quality data. For high-frequency trading, primary data (Open, High, Low, Close, Volume) and secondary data (e.g., tweets about a company, news reports) can both be used to generate buy/sell signals. 
          </p>

          <p>
            Intraday primary data is freely available but in limited quantities. <a href='https://finance.yahoo.com' target="_blank">Yahoo Finance</a>, a popular free source for financial data, only offers intraday data for the last 60 days. Secondary data is even more difficult to come across in large amounts, given the difficulty in collection and correlation. 
          </p>

          <p>
            This project focuses on getting the most out of the intraday data publicly available through Yahoo Finance; we explore ways to increase the amount of training data available for a model, and to create a machine-learning task with practical applications. 
          </p>

          <!-- Third Section -->
          <h3 class="blog-post-title">Setting Up Tasks and Targets </h3>
          <hr>
          
          <p>
            The first step is to decide on the task and the targets for our neural network model. For straightforward predictions, the task can be either classification or regression. One-step ahead classification is common in academic literature; these models attempt to predict the directionality (up or down) one time-step into the future. There are two problems with this for the project at hand. 
          </p>

          <p>
            The first is that one-step ahead predictions do not translate well to the real world. They do not reflect real-world trading behaviour, and stock prices are generally very volatile. Instead, we can adopt logic laid out in <a href='https://learning.oreilly.com/library/view/advances-in-financial/9781119482086/' target="_blank">Advances in Financial Machine Learning</a> and track prices over an interval of n steps ahead. 
          </p>

          <p>
            Second, it’s very difficult to generate strong features for classification from only primary data. There just isn’t enough separation between the binary classes. It is also not very useful to know the directionality without any sense of the magnitude. We can therefore instead use regression to get a sense of both directionality and magnitude. 
          </p>

          <p>
            So our targets become the maximum and minimum closing prices over a period of n steps ahead. We set n to 6, meaning we look 6 steps into the future (where each step is a 15-minute interval, in this case). We then take the maximum and the minimum value, and these become our two output targets (i.e., we have a multi-output model). 
          </p>

          <!-- Fourth Section -->
          <h3 class="blog-post-title">Finding Intraday Data </h3>
          <hr>

          <p>
            We can access 60 days worth of intraday data on Yahoo Finance. At 15 minutes interval, a single day would generate about 26 data points, so a total of 1560 records could be gathered for a single stock (in practice, it could be less than that). We can try increasing the amount of data available by combining data for multiple stocks into a single training set. 
          </p>
          
          <p>
            However, there are extreme variations in valuation across different stocks. A very simple approach to handling this we would be to a novel approach to scaling. 
          </p>

          <p>
            In machine learning, scaling is typically performed across the entire training set, and the values used are then applied to the test set. This wouldn’t really be ideal for our time-series data, as we would be applying the same scale to very different value-sequences; scaling across the entire set would flatten our samples and erase the patterns in the data.  
          </p>

          <p>
            We can instead scale each sample individually AFTER splitting from the main time series. A full time series would consist of 1560 steps, and our samples would be 200 step "windowed" samples from that full time series. We apply min-max scaling after splitting the time series into a 200-step sample. This way, every sample is on the same scale between 0 and 1, and the samples can be pooled and used together for training. 
          </p>

           <div class='row'> 
            <!-- VISUALS TO COME LATER -->
            <div class = 'col-lg-6'>
              <img src = 'images/ts_preds/AQN_SHOP_separate.png'> 
              
            </div>
            <div class='col-lg-6'>
              <img src='images/ts_preds/AQN_SHOP_same.png'>
              
            </div>
          </div>



          <!-- Fifth Section --> 
          <h3 class="blog-post-title">Modeling </h3>
          <hr>

          <p>
            Recurrent neural networks, especially the <a href='https://en.wikipedia.org/wiki/Long_short-term_memory' target="_blank">Long-Short Term Memory layer</a>, have been shown to be a good choice for sequential data. A good baseline for financial time series predictions established in 2017 (so arguably a bit outdated) is <a href="https://www.sciencedirect.com/science/article/abs/pii/S0377221717310652" target="_blank">a single layer RNN with 25 units </a>. 
          </p>

          <button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#exampleModal">
            TensorFlow Example
          </button>

          <!-- Modal -->
          <div class="modal fade" id="exampleModal" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
            <div class="modal-dialog modal-lg modal-fullscreen-sm-down">
              <div class="modal-content">
                <div class="modal-header">
                  <h5 class="modal-title" id="exampleModalLabel">Basic LSTM Example </h5>
                  <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body">
                  <code>
                  ########## 1-- Creating the Model ##########

                  # imports can be done differently in practice if necessary/preferred
                  import tensorflow as tf
                  from tensorflow.keras.layers import LSTM, Input, Dense
                  from tensorflow.keras.models import Model 
                  from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

                  # we can use a function to define our model
                  def make_model(input_shape):
                  &nbsp;&nbsp;input_layer = Input(shape=input_shape)

                  &nbsp;&nbsp;LSTM1 = LSTM(units=25, return_sequences=False)(input_layer)

                  &nbsp;&nbsp;max_output_layer = Dense(1, name='max_output')(LSTM1)
                  &nbsp;&nbsp;min_output_layer = Dense(1, name='min_output')(LSTM1)

                  &nbsp;&nbsp;return Model(inputs=input_layer, outputs=[max_output_layer, min_output_layer])

                  test_model = make_model(input_shape=training_samples.shape[1:])
                  test_model.summary()


                  ########## 2 -- Complining and Training ##########

                  # set some basic hyperparameters
                  EPOCHS = 500
                  BATCH_SIZE = 256 # this can be dropped down, depending on size of training set 

                  MODEL_PATH = '.../lstm1_25u.h5'

                  callbacks = [
                  &nbsp;&nbsp;ModelCheckpoint(MODEL_PATH, save_best_only=True, monitor='val_loss'),
                  &nbsp;&nbsp;ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50, min_lr=0.0001),
                  &nbsp;&nbsp;EarlyStopping(monitor='val_loss', patience=100, verbose=1)
                  &nbsp;&nbsp;]

                  test_model.compile(
                  &nbsp;&nbsp;optimizer='rmsprop',
                  &nbsp;&nbsp;loss = {'max_output': tf.losses.MeanSquareError(),
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'min_output': tf.losses.MeanSquareError()}
                  &nbsp;&nbsp;metrics = [tf.metrics.MeanAbsoluteError()],
                  )

                  # if using a GPU on Google Colabs:
                  # with tf.device('/device:GPU:0'):
                  history = test_model.fit(training_samples,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;training_labels,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_size=BATCH_SIZE,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;epochs=EPOCHS,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;callbacks=callbacks,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;validation_split=0.2,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shuffle=True,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;verbose=1
                  )
                  </code>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>
          </div>


          <p class='py-3'> The model can also be extended by adding additional LSTM layers, with Dropout layers in between. However, this doesn't really add much to performance, as measured by the validation loss. Two or three layers tend to work best for this current set-up. Below are the results for a two-layer model with 50% dropout between the layers and 50 units each. </p>

           <div class='row'> 
            <!-- VISUALS TO COME LATER -->
            <div class = 'col-lg-6'>
              <img src = 'images/ts_preds/maxpreds_vs_maxtrue2.png'> 
              
            </div>
            <div class='col-lg-6'>
              <img src='images/ts_preds/minpreds_vs_mintrue2.png'>
              
            </div>
          </div>

          <p>
           The error bands are quite thick, though the general trend is there. Moreover, what we actually care about is the difference between the predicted values and the last value in the sequence; that is, the "gains" and "losses" that we can expect to see. 
         </p>

          <p>
            We can take the difference between the last value in the sequence and the predicted maximum and minimum values to get a sense of the gains and losses. We can then take the difference between those two values to get the "spread" predicted by the model. We can compare this spread against the true gain of a given sequence (i.e., the difference between the last close price and the maximum value in our n step look ahead interval). 
          </p>

          <div class='row'> 
            <!-- VISUALS TO COME LATER -->

            <div class='col-lg-8'>
              <p>
                What we see is that if we set a "threshold" value for the spread, we can eliminate or reduce the risk of a loss while still making gains. A simple strategy could therefore be: buy a stock when the predicted spread is above a certain value (determined by your risk tolerence) and sell shortly after it has hit some modest target. With the right conditions (e.g., minimal or no trading friction), this could be a profitable use of this model's (imperfect) predictions.
              </p>

              <p>
                Of course, it should also be noted that this model is not particularly accurate for a specific stock at any given time; any utility emerges in the aggregate, as part of a high-frequency strategy. 

              </p>
              
            </div>
            <div class = 'col-lg-4'>
              <img src = 'images/ts_preds/lstm_spread.png'> 
              
            </div>
          </div>



          <h3 class="blog-post-title">Future Work </h3>
          <hr>

          <p> 
            Can we do better, from a modeling stand-point? One thing that is worth exploring in future work is the application of <a href='https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'> transformers </a> and transformer elements to financial time-series modeling. I have been playing around with transformer implementations, and hope to follow up more in the future. Early results are promising!
          </p>
        </article>

    </div>




  </main>    

  </body>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-/bQdsTh/da6pkI1MST/rWKFNjaCP5gBSY4sEBT38Q/9RBh9AH40zEOg7Hlq2THRZ" crossorigin="anonymous"></script>
</html>