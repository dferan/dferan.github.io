<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-F3w7mX95PdgyTmZZMECAngseQB83DfGTowi0iMjiWaeVhAn4FJkqJByhZMI3AhiU" crossorigin="anonymous">
    <!-- Custom stylesheet -->
    <link rel='stylesheet' type='text/css' href='github_pages.css'>

    <!-- Set the Title --> 
    <title>dferan -- Text Generation, from LSTMs to Fine-Tuning GPT-2 </title>
  </head>

  <body class='lighter_bg'>

    <!-- Header -->
    <header>
      <div class="collapse bg-dark" id="navbarHeader">
        <div class="container">
          <div class="row">
            <div class="col-sm-8 col-md-7 py-4">
              <h4 class="text-white" id="description">Portfolio</h4>
              <p class="text-muted">This site contains a collection of projects related to data science and information studies. Check out the READ(about)ME <a href='about_me.html'>here</a>.</p>
            </div>
            <div class="col-sm-4 offset-md-1 py-4">
              <h4 class="text-white">Contact</h4>
              <ul class="list-unstyled">
                <li><a href="https://www.linkedin.com/in/darragh-feran-697760202" target='_blank' class="text-white">Contact me on LinkedIn</a></li>
              </ul>
            </div>
          </div>
        </div>
      </div>
      <div class="navbar navbar-dark bg-dark shadow-sm">
        <div class="container">
          <a href="index.html" class="navbar-brand d-flex align-items-center"><strong>dferan</strong></a>

          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarHeader" aria-controls="navbarHeader" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span> 
          </button>
        </div>
      </div>
    </header>

    <main class="py-5 container">

      <!-- Title Card -->
      <div class="p-4 p-md-5 mb-4 text-white rounded bg-dark">
        <div class="col-md-12 px-0 text-center">
          <h1 class="display-4 fst-italic">Text Generation, from LSTMs to Fine-Tuning GPT-2</h1>
          <!-- <img class='img-fluid' src='images/rt_recommender/tomatoes_banner.png'>  -->
          <p class="lead my-3">NLP Analysis and Text Generation with HuggingFace Transformers, working towards conditional generation of video game summaries</p>

          <a href="https://share.streamlit.io/dferan/game_gen_app/main/test_app.py" class="btn btn-primary" role="button" target='_blank' id='first_demo_link'>Demo here!</a>
          <br>
          <p class="blog-post-meta pt-4">Updated January 20222</p>
        </div>
      </div>

      <!-- Article Section -->
      <div class="container">
        <article class="blog-post">

          <!-- First Section -->
          <h3 class="blog-post-title">Introduction and Motivation</h3>
          <hr>

          <p>
            The very public success and at-home availability (thanks to <a href='https://huggingface.co/'>Huggingface</a>) of recent large-scale, cutting edge NLP models inspired me to learn about and try my hand at text generation. While there is no shortage of text data to experiment with, a recent lapse into video gaming prompted me to try using video game summaries scraped from MetaCritic (using <a href= 'https://www.selenium.dev/'>Selenium</a>, if I remember correctly). There are four parts to this project:
          </p>
          <ol>
            <li>Cleaning and EDA: Sorting out some basic data cleaning and exploring the data and generating a few quick word clouds</li>
            <li>Character-level LSTMs: A venerable “classic” (from 2017!) method of text generation. </li>
            <li>Undirected Token-level Transformers: A quick spin-up of a more successful approach to text generation</li>
            <li>Conditional Token-level Transformers: Adding keywords to training and inference to provide some rudimentary control over the model’s output</li>

          </ol>


          <!-- Second Section -->
          <h3 class="blog-post-title">I: Cleaning and EDA </h3>
          <hr>

          <div class='row'>
            <div class='col-lg-8'>
              <h4> The Data </h4>
              <p>
                The MetaCritic data covers games from 2015-2020 with at least seven reviews. Before we can put the data to use there a few issues to resolve:
              </p>
              <ul>
                <li> Null values (esp. in "publisher" and "num_players") </li>
                <li> A large number of categorical values "num_players" </li>
                <li> The user ratings should be converted to a numerical data type</li>
                <li> The release dates should be converted to a datetimes for easier hanling</li>
              </ul>
              <p> 
                Once we finish up with that, we can move onto some simple EDA and visualizations.
              </p>
            </div>

            <div class='col-lg-4'>
              <img src='images/text_gen_games/metacritic_data.png'>
              <p class='fig-caption'> The raw scraped data, in need of some work </p>
            </div>
              
          </div>

          <!-- Third Section -->
          <h4>Visualizations </h4>

          <p>
            For fun, we can take a look at the reviews quickly. To get a sense of the differences between the preferences of professional reviewers and users, we can break down their ratings by genre. 
          </p>

          <div class='row'>
            <div class='col-lg-6'>
              <img src='images/text_gen_games/critic_heatmap.png'>
              <p class='fig-caption'> Heatmap for critics</p>
            </div>


            <div class='col-lg-6'>
              <img src='images/text_gen_games/user_heatmap.png'>  
              <p class='fig-caption'> Heatmap for users</p> 
            </div>
          </div>

          <p> There are both points of agreement and disagreement here. Both critics and users seem to agree that 2015 was a particularly good year for Action RPGs. However, users tend to be more critical overall with sports games than  professional critics.</p>

          <p> There are many other questions we could answer, but we'll turn our attention to the snyopses for now.</p>

          <p> We can get a sense of the common terms used in these synopses by visualzing common terms from a simple bag of words (making sure to remove some common English stop words and a few custom additions that are unhelpful, like "players" or "experience").</p>

          <div class='row'>
            <div class='col-lg-4 py-3'>
              <img src='images/text_gen_games/unigrams.png'>
            </div>

            <div class='col-lg-4 py-3'>
              <img src='images/text_gen_games/bigrams.png'>
            </div>

            <div class='col-lg-4 py-3'>
              <img src='images/text_gen_games/trigrams.png'>
            </div>
          </div>

          <p> We can see some terms pop out that seem in line with general trends in video gaming over the past few years (e.g., "co-op"). </p>
          

          <!-- Fourth Section -->
          <h3 class="blog-post-title">II: Text Gen Basics and LSTM Models </h3>
          <hr>

          <p>
            Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) are popular and effecitve approaches to modeling capable of generating plausible new data in a variety of domains. However, in the NLP domain, Andrej Karpthay demonstrated in 2015 that <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target='_blank'>simple RNNs could also be used to generate text.</a>
          </p>

          <p>
            In the paper, RNNs with LSTM layers were used to learn a character-level language model and predict the next character given a sequence of preceding characters. The predicted character was then appended to the sequence and fed back into the model and the process repeated until a string of text had been generated.  
          </p>

          <p>
            The concept of “temperature” was important to this process; temperature controls how a character is selected from the probability distribution created by the language model. With a low-temperature, likely characters will be selected. With a high-temperature, unlikely characters have a higher chance of being selected. Low temperatures risk creating repetitive samples, while high temperatures risk gibberish. A balance is generally ideal.   
          </p>

          <p>
            We can try this ourselves with our game summaries. We need to:
          </p>

          <ol>
            <li> Get the data ready: we need to extract the summaries and filter out summaries with non-English characters (to simplify things) </li>
            <li> Create samples and labels: the summaries should then be split into samples and labels, and vectorized on a character-level</li>
            <li> Build and train a model: we next build our LSTM model and train it for multi-class classification (i.e., predicting which character comes next)</li>
            <li> Test the model's output: we can now generate some samples </li>
          </ol>

          <p> 
            See below for a TensorFlow implementation largely adapted from <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" target='_blank'> Hands-on Machine Learning</a> and <a href='https://www.oreilly.com/library/view/deep-learning-with/9781617294433/' target='_blank'> Deep Learning with Python</a>
          </p> 

          <button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#tensorFlowModal">
            TensorFlow Implementation
          </button>

          <!-- Modal -->
          <div class="modal fade" id="tensorFlowModal" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
            <div class="modal-dialog modal-lg modal-fullscreen-sm-down">
              <div class="modal-content">
                <div class="modal-header">
                  <h5 class="modal-title" id="exampleModalLabel">Example Code for CF Recommender </h5>
                  <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body">
                  <code>
                  ########## 1 -- Getting the data in shape ##########

                  import pandas as pd
                  import tensorflow as tf
                  import numpy as np
                  import random
                  import sys
                  from keras import models, layers 
                  
                  # load in the dataset
                  data = pd.read_csv('game_data_clean.csv')

                  # filter for English/ASCII only summaries
                  def is_english(summary):
                  &nbsp;&nbsp;try:
                  &nbsp;&nbsp;&nbsp;&nbsp;summary.encode(encoding='utf-8').decode('ascii')
                  &nbsp;&nbsp;except UnicodeDecodeError:
                  &nbsp;&nbsp;&nbsp;&nbsp;return False
                  &nbsp;&nbsp;else:
                  &nbsp;&nbsp;&nbsp;&nbsp;return True
                      
                  summaries = [summary for summary in data.summary if is_english(summary)]

                  # get the unique chars and create an index
                  chars = sorted(list(set("".join(summaries))))
                  char_indices = dict((char, chars.index(char)) for char in chars)


                  ########## 2 -- Creating Samples and Labels ##########

                  # decide on a number of steps and a shift (stride) length
                  n_steps = 100
                  shift = 10 

                  # extract the samples and targets
                  samples = []
                  targets = []

                  for summary in summaries:
                  &nbsp;&nbsp;for i in range(0, len(summary) - n_steps, shift):
                  &nbsp;&nbsp;&nbsp;&nbsp;samples.append(summary[i: i+n_steps])
                  &nbsp;&nbsp;&nbsp;&nbsp;targets.append(summary[i+n_steps])
                          
                  # vectorize the samples and targets with the char_index
                  x = np.zeros((len(samples), n_steps, len(chars)), dtype=np.bool)      
                  y = np.zeros((len(samples), len(chars)), dtype=np.bool)              
                  for i, sentence in enumerate(samples):                               
                  &nbsp;&nbsp;for t, char in enumerate(sentence):                                
                  &nbsp;&nbsp;&nbsp;&nbsp;x[i, t, char_indices[char]] = 1                                
                  &nbsp;&nbsp;y[i, char_indices[targets[i]]] = 1   


                  ########## 3 -- Building and Training a Model ##########

                  # create a model using the functional API
                  def make_model(input_shape):
                      
                  &nbsp;&nbsp;# set the input
                  &nbsp;&nbsp;input_layer = layers.Input(shape=input_shape)
                      
                  &nbsp;&nbsp;# first layer
                  &nbsp;&nbsp;LSTM1 = layers.LSTM(units=128, return_sequences=True)(input_layer)
                  &nbsp;&nbsp;LSTM1 = layers.Dropout(0.5)(LSTM1)
                      
                  &nbsp;&nbsp;LSTM2 = layers.LSTM(units=128, return_sequences=False)(LSTM1)
                  &nbsp;&nbsp;LSTM2 = layers.Dropout(0.5)(LSTM2)
                      
                  &nbsp;&nbsp;output_layer = layers.Dense(len(chars), activation='softmax')(LSTM2)
                      
                  &nbsp;&nbsp;return models.Model(inputs=input_layer, outputs=output_layer)

                  test_model = make_model(input_shape=x.shape[1:])
                  #test_model.summary() # check the model if having doubts

                  # set a batch size and the number of epochs
                  batch_size = 32
                  epochs=60

                  test_model.compile(
                  &nbsp;&nbsp;optimizer = 'rmsprop',
                   &nbsp;&nbsp;loss='categorical_crossentropy')

                  history = test_model.fit(x,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_size=batch_size,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;epochs=epochs,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;validation_data=None,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shuffle=True,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;verbose=1
                                          )



                  ########## 4 -- Testing the Model's Output ##########
                  import sys

                  # create a sampling function
                  def sample(preds, temperature):
                  &nbsp;&nbsp;preds = np.asarray(preds).astype('float64') # get the inital prob dist
                  &nbsp;&nbsp;preds = np.log(preds) / temperature # redistribute based on temp
                  &nbsp;&nbsp;exp_preds = np.exp(preds)
                  &nbsp;&nbsp;preds = exp_preds / np.sum(exp_preds) # normalize
                  &nbsp;&nbsp;probas = np.random.multinomial(1, preds, 1)
                      
                  &nbsp;&nbsp;return np.argmax(probas)

                  # set a seed for the model to work with
                  seed = 'Gorplox Adventure is a brand new experience from Quest Games. Set in a'
                  seed = seed.rjust((100-len(seed)), " ")

                  # set the length of the output string
                  string_length = 150

                  # get a set of output strings at different temperatures to compare
                  for temperature in [0.2, 0.5, 1.0, 1.2]:
                  &nbsp;&nbsp;print(f'------- Temperature: {temperature}')
                  &nbsp;&nbsp;sys.stdout.write(fixed_seed)
                  &nbsp;&nbsp;generated_text = ''.join(fixed_seed)
                      
                  &nbsp;&nbsp;for i in range(string_length):
                  &nbsp;&nbsp;&nbsp;&nbsp;sampled = np.zeros((1, n_steps, len(chars)))
                  &nbsp;&nbsp;&nbsp;&nbsp;for t, char in enumerate(generated_text):
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sampled[0, t, char_indices[char]] = 1.
                              
                 &nbsp;&nbsp;&nbsp;&nbsp;preds = test_model.predict(samples, verbose=0)[0]
                 &nbsp;&nbsp;&nbsp;&nbsp;next_index = sample(preds, temperature)
                 &nbsp;&nbsp;&nbsp;&nbsp;next_char = char[next_index]
                          
                 &nbsp;&nbsp;&nbsp;&nbsp;generated_text += next_char
                 &nbsp;&nbsp;&nbsp;&nbsp;generated_text = generated_text[1:]
                          
                 &nbsp;&nbsp;&nbsp;&nbsp;sys.stdout.write(next_char)
                  </code>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>
          </div>

          <p class='pt-3'>
            At temperatures above 0.5, we tend to get nothing especially comprehensible, and at temperatures of around 0.2 the model has a tendency to fall into a repetitive loop. However, at around 0.5, we can sometimes get output strings that are roughly coherent. For example, with the seed string "Glorbox Adventure 3D is", we get the following: 
          </p>

          <p class='textGen-results'>
            Glorbox Aventure 3D is a post-ashem and Face and story and the lives of the world. Explore a special world choices a group of colories in the classic story-based gameplay
          </p>

          <p> 
            While this is unlikely to fool a human reader, it is still fairly impressive given that the model was tasked with learning English with only about 170,000 character strings.
          </p>

          <p>
            We can do better though. 
          </p>

          <!-- Fifth Section --> 
          <h3 class="blog-post-title">III: The Rise of the Transformers</h3>
          <hr>

          <p>
            <a href='https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)' target='_blank'> Transfomers </a> are a huge leap forward for NLP. While RNNs struggled with locating the relevant sections of an input sequence on especially long sequences, transformers use <a href='https://en.wikipedia.org/wiki/Attention_(machine_learning)' target="_blank"> attention mechanisms </a> to find relevant portions of the sequence. 
          </p>

          <p>
            Transformer models trained on tons of data have been able to achieve remarkable performance on a range of NLP tasks. Of particular relevance for us now is <a href='https://en.wikipedia.org/wiki/GPT-2' target="_blank"> OpenAI's GPT-2 model. </a> It has been shown to be capable of generating <a href='https://openai.com/blog/better-language-models/' target='_blank'>plausible text</a>, and with some fine-tuning, we can adapt it to our ends as well.  
          </p>

          <p>
            The easiest way to do so is with the <a href='https://huggingface.co/transformers/' target='_blank'>HuggingFace Transformers library</a>. With APIs for accessing state-of-the-art models, it's very easy to get a fine-tuned model up and running with HuggingFace. Below is an example implementation using HuggingFace and Pytorch.
          </p>

          <button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#huggingFaceModal">
            HuggingFace/Pytorch Implementation
          </button>

          <!-- Modal -->
          <div class="modal fade" id="huggingFaceModal" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
            <div class="modal-dialog modal-lg modal-fullscreen-sm-down">
              <div class="modal-content">
                <div class="modal-header">
                  <h5 class="modal-title" id="exampleModalLabel">Example Code for CF Recommender </h5>
                  <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body">
                  <code>
                  ########## 1 -- Getting the data in shape ##########

                  ### This part stays roughly the same!
                  import torch
                  import pandas as pd
                  from transformers import AutoTokenizer
                  from torch.utils.data import Dataset
                  from transformers import GPT2LMHeadModel, Trainer, TrainingArguments

                  
                  # load in the dataset
                  data = pd.read_csv('game_data_clean.csv')

                  # filter for English/ASCII only summaries
                  def is_english(summary):
                  &nbsp;&nbsp;try:
                  &nbsp;&nbsp;&nbsp;&nbsp;summary.encode(encoding='utf-8').decode('ascii')
                  &nbsp;&nbsp;except UnicodeDecodeError:
                  &nbsp;&nbsp;&nbsp;&nbsp;return False
                  &nbsp;&nbsp;else:
                  &nbsp;&nbsp;&nbsp;&nbsp;return True
                      
                  summaries = [summary for summary in data.summary if is_english(summary)]

                  # however, this time not char-level but word-level tokenizing


                  ########## 2 -- Creating the Dataset ##########

                  # load in the correct tokenizer
                  tokenizer = AutoTokenizer.from_pretrained('gpt2')
                  tokenizer.pad_token = tokenizer.eos_token

                  # set a length; we'll go for longer this time
                  MAX_LENGTH = 200

                  # create a custom Dataset class
                  class SummaryData(Dataset):
                  &nbsp;&nbsp;def __init__(self, data, tokenizer, gpt2_type = 'gpt2', max_length = MAX_LENGTH):
                  &nbsp;&nbsp;&nbsp;&nbsp;self.tokenizer = tokenizer
                  &nbsp;&nbsp;&nbsp;&nbsp;self.max_length = max_length
                  &nbsp;&nbsp;&nbsp;&nbsp;self.input_ids = []
                  &nbsp;&nbsp;&nbsp;&nbsp;self.attn_mask = []

                  &nbsp;&nbsp;&nbsp;&nbsp;for item in data:
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;encodings_dict = tokenizer('<|startoftext|>'+item+'<|endoftext|>',
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;truncation=True,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_length=self.max_length,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;padding='max_length')
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.input_ids.append(torch.tensor(encodings_dict['input_ids']))
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.attn_mask.append(torch.tensor(encodings_dict['attention_mask']))

                  &nbsp;&nbsp;def __len__(self):
                  &nbsp;&nbsp;&nbsp;&nbsp;return len(self.input_ids)

                  &nbsp;&nbsp;def __getitem__(self, idx):
                  &nbsp;&nbsp;&nbsp;&nbsp;return self.input_ids[idx], self.attn_mask[idx]

                  # instantiate the dataset
                  dataset = SummaryData(summaries, tokenizer)


                  ########## 3 -- Loading in and Fine-tuning the Model ##########

                  # load in model
                  model = GPT2LMHeadModel.from_pretrained('gpt2')

                  training_args = TrainingArguments(
                  &nbsp;&nbsp;&nbsp;&nbsp;output_dir = '/results',
                  &nbsp;&nbsp;&nbsp;&nbsp;num_train_epochs=4,
                  &nbsp;&nbsp;&nbsp;&nbsp;per_device_train_batch_size = 8,
                  &nbsp;&nbsp;&nbsp;&nbsp;warmup_steps = 100,
                  &nbsp;&nbsp;&nbsp;&nbsp;weight_decay = 0.01,
                  &nbsp;&nbsp;&nbsp;&nbsp;#max_steps = 5,
                  &nbsp;&nbsp;&nbsp;&nbsp;logging_dir = '/results'
                  )

                  def dummy_data_collector(features):
                  &nbsp;&nbsp;batch = {}
                  &nbsp;&nbsp;batch['input_ids'] = torch.stack([f[0] for f in features])
                  &nbsp;&nbsp;batch['attention_mask'] = torch.stack([f[1] for f in features])
                  &nbsp;&nbsp;batch['labels'] = batch['input_ids']
                  &nbsp;&nbsp;#result["labels"] = result["input_ids"].copy()
                      
                  &nbsp;&nbsp;return batch

                  trainer = Trainer(model = model,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;args = training_args,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train_dataset = dataset,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data_collator = dummy_data_collector)

                  # finally train the model
                  trainer.train()


                  ########## 4 -- Testing the Model's Output ##########

                  # set a seed and tokenize it
                  seed = "In a world where"

                  seed_tokens = tokenizer.encode(seed, return_tensors='pt').to(device)

                  # generate text until the output length (which includes the seed length) reaches 50
                  greedy_output = model.generate(seed_tokens, max_length=400, do_sample=True,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;top_k = 50,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#temperature = 0.7,
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;no_repeat_ngram_size=3)

                  print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))
                  </code>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>
          </div>
          
          <p class='pt-3'>
            With this model, we get much more coherent results. For example, with the seed from before we got:
          </p>

          <p class='textGen-results'>
            Glorbox Adventure 3D is a 3D platformer with a unique gameplay style. You play as a lone wolf, who is tasked with hunting down and killing all the remaining humans on the planet.
          </p>
          
          <p>
            With the seed "In a world where", we get: 
          </p>

          <p class='textGen-results'> 
            In a world where machines can do whatever they want with the digital rights, robots will still be everywhere as the most advanced forms of entertainment technology has replaced human humans in the technology's future. It doesn't take an anthropologist to see this as inevitable, as the future is more dystopian than ever.
          </p>


          <p> We can right away see that this model does a better job with syntax and grammer. This text is much more plausible, and in fact the first example could be a real game. </p>

          <p> However, we still lack control over the output; there is no real way to encourage the model to output a summary for a particular kind of game (e.g., an action game, or an RPG). </p> 

          <h3 class="blog-post-title">IV: Conditional Text Generation</h3>
          <hr>
          <p>
            There are a variety of approaches to controlling the output of a generative LM, however we will go for something simple: <a target='_blank' href='https://towardsdatascience.com/conditional-text-generation-by-fine-tuning-gpt-2-11c1a9fc639d'>conditioning our samples on a set of keywords</a>. The basic idea is to have the model learn next-token probability distributions based on the preceding tokens as well as a set of keywords prepended to the sequence.  
          </p>

          <p> For our keywords, we will keep it very simple and just the genres that a game has been labelled with. Since MetaCritic apparently has over 120 genres, we will filter out genres that apply to fewer than 100 games. This leaves us with around 30 options.</p>


          <div class='row'>
            <div class = 'col-lg-6'>
              <p>  The only real difference in approach here is that we add the keywords to the training samples before tokenizing. After training the model, I decided to make it into a Streamlit app for easier inference. </p>

              <a href="https://share.streamlit.io/dferan/game_gen_app/main/test_app.py" class="btn btn-primary" role="button" target='_blank' id='first_demo_link'>Try it out now!</a>

              <p>
                The app interface makes it easy to play around with the settings, keywords and optional initial strings. Some hand-picked example results can be seen below. 
              </p>
            </div>

            <div class='col-lg-6'>
              <img src='images/text_gen_games/interface.png'>
            </div>
          </div>


          <div class='row'>
            <div class='col-lg-6'>
              <img src='images/text_gen_games/survival_ow_shooter.png'>
              <p class='fig-caption'> Results for "Survival", "Open-World", "Shooter"</p>
            </div>


            <div class='col-lg-6'>
              <img src='images/text_gen_games/simulation_sports.png'>  
              <p class='fig-caption'> Results for "Simulation", "Sports"</p> 
            </div>
          </div>

          <p> The keywords in this case do in fact work to add a measure of control to the model output. </p>



          <h3 class="blog-post-title">Conclusions and Future Work</h3>
          <hr>
          <p>
            We worked towards being able to generate novel game summaries in particular genres, and we achieved some measure of success. However, there are of course still some issues with our model's output. 
          </p>
          <p>
            First, we did not attempt to learn disentangled representations, meaning that our control over the genres is going to be limited (e.g., generating a shooter that is not an action game is probably not feasible, but may be desierable). Further, the model showed a tendency to repeat titles with little inventiveness. 
          </p>
          <p>
            Nonetheless, even in its current state, it is plausibly useful as a tool for quickly brainstorming ideas for future projects. Moreover, the approaches here can be extended to other data, areas and applications as well (e.g., possibly neural style transfer). 
          </p>

        </article>

    </div>




  </main>    

  </body>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-/bQdsTh/da6pkI1MST/rWKFNjaCP5gBSY4sEBT38Q/9RBh9AH40zEOg7Hlq2THRZ" crossorigin="anonymous"></script>
</html>